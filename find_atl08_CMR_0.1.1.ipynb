{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b0024cd-545d-412b-9920-5550ff633c10",
   "metadata": {},
   "source": [
    "# Search EarthData's Common Metadata Repository by a bbox to get local ADAPT paths of ATL08 granules \n",
    "To find the ATL08 data you need to run through extraction and filtering, use the list returned from this noteboook \n",
    "\n",
    "Paul Montesano, Caleb Spadlin  \n",
    "September 2023\n",
    "\n",
    "### To run `do_extract_atl08_v005.py` like this:\n",
    "```[pmontesa@adaptlogin101 ~]$ pdsh -g forest do_extract_filter_atl08.sh \\\"2018 2019 2020 2021 2022 2023\\\" /explore/nobackup/people/pmontesa/userfs02/data/icesat2/atl08.006/senegal_20m/list_atl08.006_senegal senegal_20m /explore/nobackup/people/pmontesa/userfs02/data/icesat2/atl08.006```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "insured-pension",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/panfs/ccds02/home/pmontesa/code/geoscitools/atl08lib.py:1: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import certifi\n",
    "import urllib3\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/pmontesa/code/geoscitools')\n",
    "import atl08lib\n",
    "import maplib\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cultural-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# class CmrProcess\n",
    "#\n",
    "# @author: Caleb Spradlin, caleb.s.spradlin@nasa.gov\n",
    "# @version: 12.30.2021\n",
    "#\n",
    "# https://cmr.earthdata.nasa.gov/search/\n",
    "# https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html\n",
    "# -----------------------------------------------------------------------------\n",
    "class CmrProcess(object):\n",
    "\n",
    "    CMR_BASE_URL = 'https://cmr.earthdata.nasa.gov' +\\\n",
    "        '/search/granules.umm_json_v1_4?'\n",
    "\n",
    "    # Range for valid lon/lat\n",
    "    LATITUDE_RANGE = (-90, 90)\n",
    "    LONGITUDE_RANGE = (-180, 180)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # __init__\n",
    "    # -------------------------------------------------------------------------\n",
    "    def __init__(self,\n",
    "                 mission,\n",
    "                 dateTime,\n",
    "                 lonLat=None,\n",
    "                 error=False,\n",
    "                 dayNightFlag='',\n",
    "                 pageSize=150,\n",
    "                 maxPages=50):\n",
    "\n",
    "        self._error = error\n",
    "        self._dateTime = dateTime\n",
    "        self._mission = mission\n",
    "        self._pageSize = pageSize\n",
    "        self._maxPages = maxPages\n",
    "        \n",
    "        self._lonLat = lonLat\n",
    "        self._dayNightFlag = dayNightFlag\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # run()\n",
    "    #\n",
    "    # Given a set of parameters on init (time, location, mission), search for\n",
    "    # the most relevant file. This uses CMR to search metadata for\n",
    "    # relevant matches.\n",
    "    # -------------------------------------------------------------------------\n",
    "    def run(self):\n",
    "        print('Starting query')\n",
    "        outout = set()\n",
    "        for i in range(self._maxPages):\n",
    "            \n",
    "            d, e = self._cmrQuery(pageNum=i+1)\n",
    "            \n",
    "            if e and i > 1:\n",
    "                return sorted(list(outout))\n",
    "            \n",
    "            if not e:\n",
    "                print('Results found on page: {}'.format(i+1))\n",
    "                out = [r['file_url'] for r in d.values()]\n",
    "                outout.update(out)\n",
    "                \n",
    "        outout = sorted(list(outout))\n",
    "        return outout\n",
    "        \n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # cmrQuery()\n",
    "    #\n",
    "    # Search the Common Metadata Repository(CMR) for a file that\n",
    "    # is a temporal and spatial match.\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _cmrQuery(self, pageNum=1):\n",
    "\n",
    "        requestDictionary = self._buildRequest(pageNum=pageNum)\n",
    "        totalHits, resultDictionary = self._sendRequest(requestDictionary)\n",
    "\n",
    "        if self._error:\n",
    "            return None, self._error\n",
    "\n",
    "        if totalHits <= 0:\n",
    "            print('No hits on page number: {}, ending search.'.format(pageNum))\n",
    "            #warnings.warn(msg)\n",
    "            return None, True\n",
    "\n",
    "        resultDictionaryProcessed = self._processRequest(resultDictionary)\n",
    "        return resultDictionaryProcessed, self._error\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # buildRequest()\n",
    "    #\n",
    "    # Build a dictionary based off of parameters given on init.\n",
    "    # This dictionary will be used to encode the http request to search\n",
    "    # CMR.\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _buildRequest(self, pageNum=1):\n",
    "        requestDict = dict()\n",
    "        requestDict['page_num'] = pageNum\n",
    "        requestDict['page_size'] = self._pageSize\n",
    "        requestDict['concept_id'] = self._mission\n",
    "        requestDict['bounding_box'] = self._lonLat\n",
    "        requestDict['day_night_flag'] = self._dayNightFlag\n",
    "        requestDict['temporal'] = self._dateTime\n",
    "        return requestDict\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # _sendRequest\n",
    "    #\n",
    "    # Send an http request to the CMR server.\n",
    "    # Decode data and count number of hits from request.\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _sendRequest(self, requestDictionary):\n",
    "        with urllib3.PoolManager(cert_reqs='CERT_REQUIRED',\n",
    "                                 ca_certs=certifi.where()) as httpPoolManager:\n",
    "            encodedParameters = urlencode(requestDictionary, doseq=True)\n",
    "            requestUrl = self.CMR_BASE_URL + encodedParameters\n",
    "            try:\n",
    "                requestResultPackage = httpPoolManager.request('GET',\n",
    "                                                               requestUrl)\n",
    "            except urllib3.exceptions.MaxRetryError:\n",
    "                self._error = True\n",
    "                return 0, None\n",
    "\n",
    "            requestResultData = json.loads(\n",
    "                requestResultPackage.data.decode('utf-8'))\n",
    "            status = int(requestResultPackage.status)\n",
    "\n",
    "            if not status == 400:\n",
    "                totalHits = len(requestResultData['items'])\n",
    "                return totalHits, requestResultData\n",
    "\n",
    "            else:\n",
    "                msg = 'CMR Query: Client or server error: ' + \\\n",
    "                    'Status: {}, Request URL: {}, Params: {}'.format(\n",
    "                        str(status), requestUrl, encodedParameters)\n",
    "                warnings.warn(msg)\n",
    "                return 0, None\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # _processRequest\n",
    "    #\n",
    "    # For each result in the CMR query, unpackage relevant information to\n",
    "    # a dictionary. While doing so set flags if data is not desirable (too\n",
    "    # close to edge of dataset).\n",
    "    #\n",
    "    #  REVIEW: Make the hard-coded names class constants? There are a lot...\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _processRequest(self, resultDict):\n",
    "\n",
    "        resultDictProcessed = dict()\n",
    "\n",
    "        for hit in resultDict['items']:\n",
    "\n",
    "            fileName = hit['umm']['RelatedUrls'][0]['URL'].split(\n",
    "                '/')[-1]\n",
    "\n",
    "            # ---\n",
    "            # These are hardcoded here because the only time these names will\n",
    "            # ever change is if we changed which format of metadata we wanted\n",
    "            # the CMR results back in.\n",
    "            #\n",
    "            # These could be placed as class constants in the future.\n",
    "            # ---\n",
    "            fileUrl = hit['umm']['RelatedUrls'][0]['URL']\n",
    "            temporalRange = hit['umm']['TemporalExtent']['RangeDateTime']\n",
    "            dayNight = hit['umm']['DataGranule']['DayNightFlag']\n",
    "\n",
    " \n",
    "            spatialExtent = hit['umm']['SpatialExten' +\n",
    "                                          't']['HorizontalSpatialDom' +\n",
    "                                               'ain']\n",
    "\n",
    "            key = fileName\n",
    "\n",
    "            resultDictProcessed[key] = {\n",
    "                'file_name': fileName,\n",
    "                'file_url': fileUrl,\n",
    "                'temporal_range': temporalRange,\n",
    "                'spatial_extent': spatialExtent,\n",
    "                'day_night_flag': dayNight}\n",
    "\n",
    "        return resultDictProcessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-tackle",
   "metadata": {},
   "source": [
    "#### Boreal NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "closed-compromise",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dict_borealna = {\n",
    "    'site_name': 'Boreal North America',\n",
    "    'bbox': [-165,50,-45,71], \n",
    "    'minmonth': \"06\",\n",
    "    'maxmonth': \"09\",\n",
    "    'years_list': [2018,2019,2020,2021,2022]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-press",
   "metadata": {},
   "source": [
    "#### Senegal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "piano-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dict_senegal = {\n",
    "    'site_name': 'Senegal',\n",
    "    'bbox': [-18,12,-11,17], \n",
    "    'minmonth': \"01\",\n",
    "    'maxmonth': \"12\",\n",
    "    'years_list': [2018,2019,2020,2021,2022,2023,2024]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "29b85f10-4780-43c1-a5f7-9e3330852513",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dict_howland = {\n",
    "    'site_name': 'Howland',\n",
    "    'bbox': [-69,44,-68,46], \n",
    "    'minmonth': \"06\",\n",
    "    'maxmonth': \"09\",\n",
    "    'years_list': [2018,2019,2020,2021,2022]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1f8629eb-5e61-4981-941a-17e9152651f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dict_serc = {\n",
    "    'site_name': 'SERC',\n",
    "    'bbox': [-76.6,38.8,-76.5,38.9], \n",
    "    'minmonth': \"06\",\n",
    "    'maxmonth': \"09\",\n",
    "    'years_list': [2018,2019,2020,2021,2022]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-convergence",
   "metadata": {},
   "source": [
    "#### Bhasan Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "streaming-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dict_bhasan = {\n",
    "    'site_name': 'Bhasan Char',\n",
    "    'bbox': [91.36,22.35,91.43,22.392], \n",
    "    'minmonth': \"01\",\n",
    "    'maxmonth': \"12\",\n",
    "    'years_list': [2018,2019,2020,2021,2022]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca40fc-cd5c-4fbb-945a-e682ed5f8969",
   "metadata": {},
   "source": [
    "## Choose a site and create subdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "583a2c9a-007d-4829-b4b2-97d4a2cd5d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This provides the ATLAS data product and version - also used for subdir in ADAPT/EXPLORE\n",
    "ATL08_VERSION = 'ATL08.006'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "stable-acrylic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/explore/nobackup/people/pmontesa/userfs02/data/icesat2/atl08.006/senegal_20m'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a site\n",
    "search_dict = search_dict_senegal\n",
    "\n",
    "# Choose a data output subdir\n",
    "SUBDIR_NAME = 'senegal_20m'\n",
    "OUTPUT_DIR = f'/explore/nobackup/people/pmontesa/userfs02/data/icesat2/{ATL08_VERSION.lower()}/{SUBDIR_NAME}'\n",
    "!mkdir -p $OUTPUT_DIR\n",
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-lewis",
   "metadata": {},
   "source": [
    "#### Build search: seasonal search across list of years for ATL08 in a bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7e31ca38-07a9-4d2e-aad3-a069e723121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPT/EXPLORE main dir for all ATLAS data\n",
    "DIR_ATLAS = '/css/icesat-2/ATLAS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "93437c9d-346e-498f-b53d-74fcd12d7586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/css/icesat-2/ATLAS/ATL08.006'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format the ATL08 data subdir needed to make ADAPT/EXPLORE specific granule path lists for 'do_extract_atl08' scripts\n",
    "PATH_ATL08 = os.path.join(DIR_ATLAS, ATL08_VERSION)\n",
    "PATH_ATL08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "13a3639d-36f8-4b77-b450-bf8b06d73813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find this at https://search.earthdata.nasa.gov/\n",
    "COLLECTIONCONCEPTID_DICT = {\n",
    "                        'ATL08.003': \"C2003772626-NSIDC_ECS\",\n",
    "                        'ATL08.005': \"C2144424132-NSIDC_ECS\",\n",
    "                        'ATL08.006': \"C2565090645-NSIDC_ECS\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "silver-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page size: 150, number of results returned by page.\n",
    "PAGESIZE = 150 \n",
    "# Max page, number of pages to return before ending query.\n",
    "MAXPAGE = 60\n",
    "# Total max results will be PAGESIZE * MAXPAG\n",
    "\n",
    "cmrP_list = []\n",
    "for YEAR in search_dict['years_list']:\n",
    "    \n",
    "    cmrP = CmrProcess(mission = COLLECTIONCONCEPTID_DICT[ATL08_VERSION], \n",
    "                      dateTime=f\"{YEAR}-{search_dict['minmonth']}-01T00:00:00Z,{YEAR}-{search_dict['maxmonth']}-30T23:59:59Z\", \n",
    "                      lonLat = ','.join(str(e) for e in search_dict['bbox']),\n",
    "                      pageSize=PAGESIZE,\n",
    "                      maxPages=MAXPAGE)\n",
    "    cmrP_list.append(cmrP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-trinity",
   "metadata": {},
   "source": [
    "#### Run search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "confident-scope",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting query\n",
      "Results found on page: 1\n",
      "No hits on page number: 2, ending search.\n",
      "No hits on page number: 3, ending search.\n",
      "Starting query\n",
      "Results found on page: 1\n",
      "Results found on page: 2\n",
      "No hits on page number: 3, ending search.\n",
      "Starting query\n",
      "Results found on page: 1\n",
      "Results found on page: 2\n",
      "No hits on page number: 3, ending search.\n",
      "Starting query\n",
      "Results found on page: 1\n",
      "Results found on page: 2\n",
      "No hits on page number: 3, ending search.\n",
      "Starting query\n",
      "Results found on page: 1\n",
      "Results found on page: 2\n",
      "No hits on page number: 3, ending search.\n",
      "Starting query\n",
      "Results found on page: 1\n",
      "No hits on page number: 2, ending search.\n",
      "No hits on page number: 3, ending search.\n",
      "Starting query\n",
      "No hits on page number: 1, ending search.\n",
      "No hits on page number: 2, ending search.\n",
      "No hits on page number: 3, ending search.\n"
     ]
    }
   ],
   "source": [
    "resultList_year = [cmrP.run() for cmrP in cmrP_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fcebec6a-4145-4d29-9b1d-2be0c7befa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results = dict(zip(search_dict['years_list'], resultList_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5a64f973-d00a-4ed8-8f57-efcbb096f426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ATL08.006 granules by year:\n",
      "2018: 38\n",
      "2019: 206\n",
      "2020: 206\n",
      "2021: 197\n",
      "2022: 199\n",
      "2023: 98\n",
      "2024: 0\n",
      "\n",
      "944 total granules in search results list\n",
      "\n",
      "Note: # of filtered ATL08 CSV files will be a subset of this total.\n"
     ]
    }
   ],
   "source": [
    "print(f'# {ATL08_VERSION} granules by year:')\n",
    "[print(f'{YEAR}: {len(dict_results[YEAR])}') for YEAR in search_dict['years_list']]\n",
    "\n",
    "# Get one large list with all years\n",
    "atl08_granule_list = [item for sublist in resultList_year for item in sublist]\n",
    "print(f\"\\n{len(atl08_granule_list)} total granules in search results list\")\n",
    "\n",
    "print('\\nNote: # of filtered ATL08 CSV files will be a subset of this total.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "28172747-c777-4792-88f4-6f85e9c68b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Get list of just granule names without paths\n",
    "    [g.split('/')[-1] for g in atl08_granule_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-recycling",
   "metadata": {},
   "source": [
    "#### Here is a dataframe of the granule paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "cheap-burner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_cmr</th>\n",
       "      <th>site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...</td>\n",
       "      <td>Senegal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...</td>\n",
       "      <td>Senegal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...</td>\n",
       "      <td>Senegal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...</td>\n",
       "      <td>Senegal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...</td>\n",
       "      <td>Senegal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            path_cmr site_name\n",
       "0  https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...   Senegal\n",
       "1  https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...   Senegal\n",
       "2  https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...   Senegal\n",
       "3  https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...   Senegal\n",
       "4  https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...   Senegal"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atl08_h5_fn_df = pd.DataFrame(atl08_granule_list, columns = ['path_cmr'])\n",
    "atl08_h5_fn_df['site_name'] = search_dict['site_name']\n",
    "atl08_h5_fn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0489e3-d237-4d3d-bdbb-ee9e618d601b",
   "metadata": {},
   "source": [
    "### Get full local paths of the ATL08 data for granule lists needed for `do_extract_atl08`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2d449612-56c0-4ccb-847f-076c1eae9b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/css/icesat-2/ATLAS/ATL08.006/2018.10.16/ATL08_20181016043523_02680107_006_02.h5'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atl08_h5_fn_df[\"path_local\"] = PATH_ATL08\n",
    "atl08_h5_fn_df[\"file\"] = atl08_h5_fn_df[\"path_cmr\"].apply(lambda x: os.path.basename(x))\n",
    "atl08_h5_fn_df[\"date_subdir\"] = pd.to_datetime(atl08_h5_fn_df[\"file\"].str.split('_', expand=True)[1].str[:8] , format=\"%Y%m%d\").dt.strftime('%Y.%m.%d')\n",
    "atl08_h5_fn_df[\"year\"] = atl08_h5_fn_df[\"file\"].str.split('_', expand=True)[1].str[:4]\n",
    "atl08_h5_fn_df[\"path_local\"] = atl08_h5_fn_df[['path_local', 'date_subdir', 'file']].apply(lambda row: os.path.join(*row), axis=1)\n",
    "\n",
    "atl08_h5_fn_df.path_local.to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5187e490-4cb9-4d1f-a8f9-595ccd00d4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_cmr</th>\n",
       "      <th>site_name</th>\n",
       "      <th>path_local</th>\n",
       "      <th>file</th>\n",
       "      <th>date_subdir</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...</td>\n",
       "      <td>Senegal</td>\n",
       "      <td>/css/icesat-2/ATLAS/ATL08.006/2018.10.16/ATL08...</td>\n",
       "      <td>ATL08_20181016043523_02680107_006_02.h5</td>\n",
       "      <td>2018.10.16</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...</td>\n",
       "      <td>Senegal</td>\n",
       "      <td>/css/icesat-2/ATLAS/ATL08.006/2018.10.16/ATL08...</td>\n",
       "      <td>ATL08_20181016162940_02760101_006_02.h5</td>\n",
       "      <td>2018.10.16</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...</td>\n",
       "      <td>Senegal</td>\n",
       "      <td>/css/icesat-2/ATLAS/ATL08.006/2018.10.17/ATL08...</td>\n",
       "      <td>ATL08_20181017040943_02830107_006_02.h5</td>\n",
       "      <td>2018.10.17</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...</td>\n",
       "      <td>Senegal</td>\n",
       "      <td>/css/icesat-2/ATLAS/ATL08.006/2018.10.21/ATL08...</td>\n",
       "      <td>ATL08_20181021040123_03440107_006_02.h5</td>\n",
       "      <td>2018.10.21</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...</td>\n",
       "      <td>Senegal</td>\n",
       "      <td>/css/icesat-2/ATLAS/ATL08.006/2018.10.21/ATL08...</td>\n",
       "      <td>ATL08_20181021155541_03520101_006_02.h5</td>\n",
       "      <td>2018.10.21</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            path_cmr site_name  \\\n",
       "0  https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...   Senegal   \n",
       "1  https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...   Senegal   \n",
       "2  https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...   Senegal   \n",
       "3  https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...   Senegal   \n",
       "4  https://n5eil01u.ecs.nsidc.org/DP5/ATLAS/ATL08...   Senegal   \n",
       "\n",
       "                                          path_local  \\\n",
       "0  /css/icesat-2/ATLAS/ATL08.006/2018.10.16/ATL08...   \n",
       "1  /css/icesat-2/ATLAS/ATL08.006/2018.10.16/ATL08...   \n",
       "2  /css/icesat-2/ATLAS/ATL08.006/2018.10.17/ATL08...   \n",
       "3  /css/icesat-2/ATLAS/ATL08.006/2018.10.21/ATL08...   \n",
       "4  /css/icesat-2/ATLAS/ATL08.006/2018.10.21/ATL08...   \n",
       "\n",
       "                                      file date_subdir  year  \n",
       "0  ATL08_20181016043523_02680107_006_02.h5  2018.10.16  2018  \n",
       "1  ATL08_20181016162940_02760101_006_02.h5  2018.10.16  2018  \n",
       "2  ATL08_20181017040943_02830107_006_02.h5  2018.10.17  2018  \n",
       "3  ATL08_20181021040123_03440107_006_02.h5  2018.10.21  2018  \n",
       "4  ATL08_20181021155541_03520101_006_02.h5  2018.10.21  2018  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atl08_h5_fn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-detroit",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1. Can we get these as s3 paths? ---> probably not yet on AWS\n",
    "2. Should we download to our AWS s3 buckets and store? or just transfer from ADAPT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bbad04-a3bc-4fca-888c-26336f97d4c3",
   "metadata": {},
   "source": [
    "### Write a text file of the list of graules with full local path\n",
    "this will be the main list that can be split up by VM to parallelize for `do_extract_filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "da2d26df-a6f2-4730-8439-819cf2ca86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_nodes_fn = f'{OUTPUT_DIR}/nodelist'\n",
    "LIST_VM = ['forest201', 'forest202', 'forest203', 'forest204', #'forest206',\n",
    "           'forest206','forest207','forest208','forest209','forest210']\n",
    "np.savetxt(list_nodes_fn, LIST_VM, delimiter=\"\\n\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190a340-582b-4a2f-8905-3ad8bec23088",
   "metadata": {},
   "source": [
    "## Generate sublists (chunks) by YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d2ebd91e-d92f-46a6-9e8c-e873cd2330d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "forest201\n",
      "forest202\n",
      "forest203\n",
      "forest204\n",
      "forest206\n",
      "forest207\n",
      "forest208\n",
      "forest209\n",
      "forest210\n",
      "# lines: 38, # VMs: 9\n",
      "Chunksize = 4\n",
      "2019\n",
      "forest201\n",
      "forest202\n",
      "forest203\n",
      "forest204\n",
      "forest206\n",
      "forest207\n",
      "forest208\n",
      "forest209\n",
      "forest210\n",
      "# lines: 206, # VMs: 9\n",
      "Chunksize = 22\n",
      "2020\n",
      "forest201\n",
      "forest202\n",
      "forest203\n",
      "forest204\n",
      "forest206\n",
      "forest207\n",
      "forest208\n",
      "forest209\n",
      "forest210\n",
      "# lines: 206, # VMs: 9\n",
      "Chunksize = 22\n",
      "2021\n",
      "forest201\n",
      "forest202\n",
      "forest203\n",
      "forest204\n",
      "forest206\n",
      "forest207\n",
      "forest208\n",
      "forest209\n",
      "forest210\n",
      "# lines: 197, # VMs: 9\n",
      "Chunksize = 21\n",
      "2022\n",
      "forest201\n",
      "forest202\n",
      "forest203\n",
      "forest204\n",
      "forest206\n",
      "forest207\n",
      "forest208\n",
      "forest209\n",
      "forest210\n",
      "# lines: 199, # VMs: 9\n",
      "Chunksize = 22\n",
      "2023\n",
      "forest201\n",
      "forest202\n",
      "forest203\n",
      "forest204\n",
      "forest206\n",
      "forest207\n",
      "forest208\n",
      "forest209\n",
      "forest210\n",
      "# lines: 98, # VMs: 9\n",
      "Chunksize = 10\n",
      "2024\n",
      "forest201\n",
      "forest202\n",
      "forest203\n",
      "forest204\n",
      "forest206\n",
      "forest207\n",
      "forest208\n",
      "forest209\n",
      "forest210\n",
      "# lines: 0, # VMs: 9\n",
      "Chunksize = 0\n"
     ]
    }
   ],
   "source": [
    "for YEAR in search_dict['years_list']:\n",
    "    print(YEAR)\n",
    "    list_main_fn = f\"{OUTPUT_DIR}/list_{ATL08_VERSION.lower()}_{search_dict['site_name'].replace(' ', '').lower()}_{YEAR}\"\n",
    "    df_sub = atl08_h5_fn_df[atl08_h5_fn_df.year == str(YEAR)]\n",
    "    np.savetxt(list_main_fn, df_sub.path_local, delimiter=\"\\n\", fmt=\"%s\")\n",
    "    \n",
    "    # Generate sublists (chunks) from YEAR mainlist\n",
    "    !/home/pmontesa/code/HRSI/gen_chunks.py $list_main_fn $list_nodes_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6bb8ac-009e-4a57-b677-6a4eb3c3b998",
   "metadata": {},
   "source": [
    "## Build GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5dc954fb-bb94-4df1-88ba-d694e415cf86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/explore/nobackup/people/pmontesa/userfs02/data/icesat2/atl08.006/senegal_20m/senegal_20m.gpkg'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dict['years_list']\n",
    "OUTPUT_DIR\n",
    "os.path.join(OUTPUT_DIR, filename + '.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10384917-ccb9-45be-901d-4c626696c9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building list of ATL08 csvs...\n",
      "12\n",
      "Creating pandas data frame...\n",
      "Creating a gdf for 2018 @ 20m...\n",
      "Building list of ATL08 csvs...\n",
      "57\n",
      "Creating pandas data frame...\n",
      "Creating a gdf for 2019 @ 20m...\n",
      "Building list of ATL08 csvs...\n",
      "51\n",
      "Creating pandas data frame...\n",
      "Creating a gdf for 2020 @ 20m...\n",
      "Building list of ATL08 csvs...\n",
      "44\n",
      "Creating pandas data frame...\n",
      "Creating a gdf for 2021 @ 20m...\n",
      "Building list of ATL08 csvs...\n",
      "57\n",
      "Creating pandas data frame...\n",
      "Creating a gdf for 2022 @ 20m...\n",
      "Building list of ATL08 csvs...\n",
      "30\n",
      "Creating pandas data frame...\n",
      "Creating a gdf for 2023 @ 20m...\n",
      "Building list of ATL08 csvs...\n",
      "0\n",
      "No csvs for a gdf for 2024 @ 20m.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "atl08_gdf = pd.concat([atl08lib.atl08_io(OUTPUT_DIR, str(YEAR), DO_PICKLE=False, LENGTH_SEG=20) for YEAR in search_dict['years_list'] ])\n",
    "filename = os.path.basename(OUTPUT_DIR)\n",
    "atl08_gdf.to_file(os.path.join(OUTPUT_DIR, filename + '.gpkg'), driver='GPKG')\n",
    "atl08_gdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879a290-d62c-4f68-8c99-19487e94ecfc",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d13a8-8380-4703-9903-7324c6fe4a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping 125896 day/night ATL08 observations of h_can\n",
      "Mapping unique groups in y: [2018, 2019, 2020, 2021, 2022, 2023]\n"
     ]
    }
   ],
   "source": [
    "maplib.MAP_ATL08_FOLIUM(atl08_gdf.sample(frac=0.1), MAP_COL='h_can', GROUP_COL='y', DO_NIGHT=False, RADIUS=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e24fce-6522-41d0-8ee1-8cccc30485be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ILAB Kernel",
   "language": "python",
   "name": "ilab-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
